### 01. 메모리 문제 (로 추측)
2024-06-03 10:54:48,961 - INFO - self.device is : cpu
2024-06-03 10:54:48,961 - INFO - MyMidm's load_model() START.
2024-06-03 10:54:49,586 - INFO - 토크나이저 로드 완료.
>>
Process finished with exit code -1073741819 (0xC0000005)

### 해결 시도 1. memory mapping 을 이용한 모델 로드.
    모델의 일부를 부분적으로 로드해서 메모리 사용을 줄여보려는 시도이다.
    모델의 파라미터를 부분적으로 로드하고, 필요한 부분을 메모리에 매핑한다.

#### Step 1. >> device_map="auto"
    transformers 의 AutoModelForCausalLM.from_pretrained 함수에 device_map="auto" 를 명시한다.
    이는 모델 로드 시 메모리 사용을 최적화 하고, 여러 디바이스에 분산시키는 옵션이라고 한다.
결과: >>
    ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate
    위와 같은 에러가 발생했다. 이 옵션은 디바이스(CPU, GPU) 분산이므로, CPU 만 있는 환경에서는 도움이 안되는 듯 하다.
    그래도 밑져야 본전! accelerate 를 설치하고 (pip install accelerate) , 다시 실행해보았다.
>>
    Loading checkpoint shards: 100%|██████████| 2/2 [00:00<?, ?it/s]
    ... (생략) ...
    ValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.
    전체 모델을 디스크에 offload 하려고 시도하는데, 이는 적절한 방법이 아니라고 한다.
    에러 메시지가 제안해 준 low_cpu_mem_usage=True 옵션을 사용해보겠다.
#### Step 2. >> low_cpu_mem_usage=True
결과: >>
    Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
    Process finished with exit code -1073741819 (0xC0000005)
    컴퓨터가 잠시 멈췄고 역시 문제가 있다.
    (sk의 koBERT 모델 다운로드 시에는 shards? 로 나뉘어서 다운받는 형식이 아니었는데, 이부분에 집중해보자...)

    shards 란, 대형 모델 파일을 여러 개의 작은 파일로 분할 한 것으로, 
    Huggingface 에서는 대형 모델을 여러 개의 shard로 나누어 저장한다고 한다.

shards 모델을 개별적으로 로드하는 방법: >> "snapshot"
    snapshot_download(모델명, 모델 경로, ignore_patterns=["*.h5", "*.ot", "*.onnx"])
결과: >> "한 단계 해결! "
    2024-06-03 13:07:15,094 - INFO - KT-AI/midm-bitext-S-7B-inst-v1's model download START.
    Fetching 16 files: 100%|██████████| 16/16 [08:43<00:00, 32.73s/it]
    2024-06-03 13:15:59,093 - INFO - 원본 모델 다운로드 완료.
    모델이 다운받아졌다! 작업관리자로 모니터링 시 메모리는 75%+ 가량 잡아먹고 있었다.
    그런데 다운받은 파일을 확인해보니, 
    .huggingface/download/분할된파일들.metadata
    config.json
    configuration_midm.py
    generation_config.json
    midm_bitext_tokenization.py
    midm_bitext_tokenizer.model
    model.safetensors.index.json
    model-00001-of-00002.safetensors
    model-00002-of-00002.safetensors
    modeling_midm.py
    pytorch_model.bin.index.json
    pytorch_model-00001-of-00002.bin
    pytorch_model-00002-of-00002.bin
    rotary_position_embedding.py
    tokenizer_config.json
    이와 같이 분할된 형태였다..  분할 된 형태를 다시 합쳐야 하는게 아닌가..?
>> 
    허깅페이스의 모델이 여러 shard 로 나누어 저장될 때는
    해당 shard 들을 따로 병합할 필요 없이, 모델 로드 시 자동으로 병합한다고 한다.
    transformers 의 라이브러리가 (from_pretrained 함수에서) 그 작업을 처리해준다.
    아하! 애초에 전에 써본 모델은 shard 없이 업로드되어 있었고, midm 모델은 나누어서 올려져있었나보다..! (추측,,)

    2024-06-03 13:47:42,216 - INFO - ./model/snapshot 경로의 모델을 로드 합니다.
    Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
    Process finished with exit code -1073741819 (0xC0000005)
    그러나, 모델 로드시 (분할된 shards 를 로드..) 여전히 메모리가 부족했다.
    핵 똥컴같지만 해결해보고 싶다..

#### Step 3. >> 모델 가중치 초기화, 기울기 계산 비활성화 하여 최적화 시도.
>>
    try:
        self.model = AutoModelForCausalLM.from_pretrained( ... 생략 ... 
    except Exception as e:
        with init_empty_weights():
            self.model = AutoModelForCausalLM.from_config(
                config=f"{self.local_model_path}/config.json"
                )
            load_checkpoint_in_model(
                self.model,
                checkpoint=f"{self.local_model_path}",
                device_map="auto",
                low_cpu_mem_usage=True
            )
            self.model.to(self.device)
    위와 같은 형태로 일단 기본  shards 분할로 로드 시도 후, 실패 시 exception 으로 재시도 하고 싶었는데, 수행되지 않았다.
    메모리 접근 위반 -1073741819 (0xC0000005) 문제는 파이썬 레벨에서 잡기 어렵다고 한다.
    하지만 GPT가 psutil 로 메모리를 모니터링하여, 임계치 초과 시 예외를 발생시키도록 하는 작업을 추천하여,
    시도해보고 있다.
    잘 되지 않는다..
    나는 약간 동기-비동기 식으로 
    1) task A - 모델 로드 (1회성, 오래 걸리는 작업으로 계속 단독 진행)
    2) while 문을 통해 주기적으로 메모리 모니터링
    3) 만약 메모리 모니터링 중 메모리 사용량이 임계값을 넘은 경우 task A 를 중단시킴.
    4) task B - 최적화 후 모델 로드 (1회성, 계속 단독 진행)
    5) while 문이 중단되면 task B 를 수행 시작
    이런 식으로 작업하고 싶다..

>> Python의 threading 사용






